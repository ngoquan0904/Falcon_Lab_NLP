{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":36.10645,"end_time":"2025-02-26T02:29:11.558616","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-02-26T02:28:35.452166","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"3e5aaea2","cell_type":"markdown","source":"# Import library","metadata":{"papermill":{"duration":0.005778,"end_time":"2025-02-26T02:28:55.612288","exception":false,"start_time":"2025-02-26T02:28:55.606510","status":"completed"},"tags":[]}},{"id":"48c09e60","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport json\nimport argparse,random,logging,numpy,os\nimport torch.utils.data as data\n\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils import clip_grad_norm\nfrom time import time\nfrom tqdm import tqdm\n\nfrom collections import OrderedDict\nfrom glob import glob\nfrom time import time\nfrom multiprocessing import Pool,cpu_count\nfrom itertools import chain","metadata":{"execution":{"iopub.status.busy":"2025-04-26T01:58:49.374201Z","iopub.execute_input":"2025-04-26T01:58:49.374523Z","iopub.status.idle":"2025-04-26T01:58:49.379345Z","shell.execute_reply.started":"2025-04-26T01:58:49.374492Z","shell.execute_reply":"2025-04-26T01:58:49.378662Z"},"id":"k5b1eXlFuWuB","papermill":{"duration":3.133578,"end_time":"2025-02-26T02:28:58.751902","exception":false,"start_time":"2025-02-26T02:28:55.618324","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":2},{"id":"e8b5db11","cell_type":"markdown","source":"# Build model","metadata":{"id":"zf-idKMcr9NB","papermill":{"duration":0.006363,"end_time":"2025-02-26T02:28:58.765033","exception":false,"start_time":"2025-02-26T02:28:58.758670","status":"completed"},"tags":[]}},{"id":"696ec7e6","cell_type":"markdown","source":"## Build basic model","metadata":{"id":"0RWbcxkuwXoa","papermill":{"duration":0.005763,"end_time":"2025-02-26T02:28:58.777013","exception":false,"start_time":"2025-02-26T02:28:58.771250","status":"completed"},"tags":[]}},{"id":"a3e772f4","cell_type":"code","source":"import torch\nimport os\nfrom torch.autograd import Variable\n\nclass BasicModule(torch.nn.Module):\n    def __init__(self, device, save_dir='/kaggle/working/checkpoints/', seed=1):\n        super(BasicModule, self).__init__()  # Gọi hàm khởi tạo của lớp cha (torch.nn.Module)\n        self.model_name = self.__class__.__name__  # Lưu tên của lớp hiện tại\n        self.device = device  # Thiết bị (CPU/GPU) để chạy mô hình\n        self.save_dir = save_dir  # Thư mục lưu trữ các checkpoint\n        self.seed = seed if seed is not None else 0 \n\n        # Kiểm tra và tạo thư mục nếu không tồn tại\n        if not os.path.isdir(self.save_dir):\n            os.makedirs(self.save_dir)\n\n    def save(self):\n        checkpoint = {'model': self.state_dict(), 'seed': self.seed}  # Tạo dictionary chứa trạng thái mô hình và seed\n        best_path = f'{self.save_dir}/{self.model_name}_seed_{self.seed}.pt'  # Tạo đường dẫn lưu checkpoint\n        torch.save(checkpoint, best_path)  # Lưu checkpoint\n        return best_path  # Trả về đường dẫn lưu checkpoint\n\n    def load(self, best_path):\n        if self.device is not None:\n            data = torch.load(best_path)['model']  # Tải checkpoint từ GPU/CPU\n        else:\n            data = torch.load(best_path, map_location=torch.device('cpu'))['model']  # Tải checkpoint từ CPU\n        self.load_state_dict(data)  # Load trạng thái mô hình từ checkpoint\n        return self.to(self.device)  # Chuyển mô hình sang thiết bị (CPU/GPU)\n        \n    def pad_doc(self, words_out, doc_lens):\n        pad_dim = words_out.size(1)  # Chiều kích thước của từ ngữ\n        max_doc_len = max(doc_lens)  # Độ dài tối đa của các đoạn văn bản\n        sent_input = []  # Danh sách chứa các đoạn văn bản đã padding\n        start = 0\n        for doc_len in doc_lens:\n            stop = start + doc_len\n            valid = words_out[start:stop]  # Lấy ra đoạn văn bản có độ dài tương ứng\n            start = stop\n            if doc_len == max_doc_len:\n                sent_input.append(valid.unsqueeze(0))  # Thêm đoạn văn bản vào danh sách nếu không cần padding\n            else:\n                pad = torch.zeros(max_doc_len - doc_len, pad_dim).to(self.device)  # Tạo padding cho đoạn văn bản\n                sent_input.append(torch.cat([valid, pad]).unsqueeze(0))  # Thêm đoạn văn bản đã được padding vào danh sách\n        sent_input = torch.cat(sent_input, dim=0)  # Gộp tất cả các đoạn văn bản đã padding thành một tensor\n        return sent_input\n","metadata":{"execution":{"iopub.status.busy":"2025-04-26T01:58:49.908673Z","iopub.execute_input":"2025-04-26T01:58:49.908969Z","iopub.status.idle":"2025-04-26T01:58:49.917019Z","shell.execute_reply.started":"2025-04-26T01:58:49.908946Z","shell.execute_reply":"2025-04-26T01:58:49.916157Z"},"id":"2AOxY-K_r8p1","papermill":{"duration":0.015312,"end_time":"2025-02-26T02:28:58.798245","exception":false,"start_time":"2025-02-26T02:28:58.782933","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":3},{"id":"622283f7","cell_type":"markdown","source":"## Build CNN-RNN","metadata":{"id":"MSPV6HvAwbM3","papermill":{"duration":0.005563,"end_time":"2025-02-26T02:28:58.809699","exception":false,"start_time":"2025-02-26T02:28:58.804136","status":"completed"},"tags":[]}},{"id":"8ee8d84b","cell_type":"code","source":"\nclass CNN_RNN(BasicModule):\n    def __init__(self, device, kernel_sizes, embed_dim, kernel_num, embed_num, hidden_size, seg_num, pos_num, pos_dim, embed=None):\n        super(CNN_RNN, self).__init__(device)  # Gọi hàm khởi tạo của lớp cha (BasicModule)\n\n        self.model_name = 'CNN_RNN'  # Đặt tên cho mô hình\n        self.hidden_size = hidden_size  # Kích thước lớp ẩn của RNN\n        self.device = device  # Thiết bị (CPU/GPU) để chạy mô hình\n\n        # Cài đặt thông số cho các biến\n        Ks = kernel_sizes  # Kích thước kernel của CNN\n        Ci = embed_dim  # Kích thước embedding của từ\n        Co = kernel_num  # Số lượng kernel của CNN\n        V = embed_num  # Số lượng từ trong từ điển\n        D = embed_dim  # Kích thước embedding của từ\n        H = hidden_size  # Kích thước lớp ẩn của RNN\n        S = seg_num  # Số lượng phân đoạn của văn bản\n        P_V = pos_num  # Số lượng vị trí tuyệt đối\n        P_D = pos_dim  # Kích thước embedding của vị trí\n\n        self.abs_pos_embed = nn.Embedding(P_V, P_D)  # Embedding cho vị trí tuyệt đối\n        self.rel_pos_embed = nn.Embedding(S, P_D)  # Embedding cho vị trí tương đối\n        self.embed = nn.Embedding(V, D, padding_idx=0)  # Embedding cho từ, với padding index\n        if embed is not None:\n            self.embed.weight.data.copy_(embed)  # Nếu có embedding được cung cấp, sao chép vào embedding layer\n\n        # Danh sách các lớp CNN\n        self.convs = nn.ModuleList([nn.Sequential(\n            nn.Conv1d(Ci, Co, K),  # Lớp Convolutional 1D\n            nn.BatchNorm1d(Co),  # Batch normalization\n            nn.LeakyReLU(inplace=True),  # Hàm kích hoạt Leaky ReLU\n\n            nn.Conv1d(Co, Co, K),  # Lớp Convolutional 1D thứ hai\n            nn.BatchNorm1d(Co),  # Batch normalization\n            nn.LeakyReLU(inplace=True)  # Hàm kích hoạt Leaky ReLU\n        ) for K in Ks])\n\n        # Lớp RNN (GRU)\n        self.sent_RNN = nn.GRU(\n            input_size=Co * len(Ks),\n            hidden_size=H,\n            batch_first=True,\n            bidirectional=True\n        )\n        \n        # Lớp Fully Connected\n        self.fc = nn.Sequential(\n            nn.Linear(2 * H, 2 * H),\n            nn.BatchNorm1d(2 * H),\n            nn.Tanh()\n        )\n\n        # Các lớp phân loại\n        self.content = nn.Linear(2 * H, 1, bias=False)\n        self.salience = nn.Bilinear(2 * H, 2 * H, 1, bias=False)\n        self.novelty = nn.Bilinear(2 * H, 2 * H, 1, bias=False)\n        self.abs_pos = nn.Linear(P_D, 1, bias=False)\n        self.rel_pos = nn.Linear(P_D, 1, bias=False)\n        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1, 0.1))  # Khởi tạo bias\n    \n    def max_pool1d(self, x, seq_lens):\n        # x:[N, L, O_in]  (N: batch size, L: độ dài chuỗi tối đa, O_in: số chiều kênh đầu vào)\n        out = []\n        for index, t in enumerate(x):\n            # Lấy phần dữ liệu hợp lệ của chuỗi (loại bỏ padding)\n            t = t[:seq_lens[index], :]\n            # Chuyển vị và thêm chiều để phù hợp với F.max_pool1d\n            t = torch.t(t).unsqueeze(0)\n            # Thực hiện max pooling 1D và thêm vào danh sách kết quả\n            out.append(F.max_pool1d(t, t.size(2)))\n    \n        # Ghép các kết quả và loại bỏ chiều không cần thiết\n        out = torch.cat(out).squeeze(2)\n        return out\n    \n    def avg_pool1d(self, x, seq_lens):\n        # Tương tự như max_pool1d, nhưng sử dụng average pooling\n        out = []\n        for index, t in enumerate(x):\n            t = t[:seq_lens[index], :]\n            t = torch.t(t).unsqueeze(0)\n            out.append(F.avg_pool1d(t, t.size(2)))\n    \n        out = torch.cat(out).squeeze(2)\n        return out\n    \n    def forward(self, x, doc_lens):\n        # x:[N, L, D] (N: batch size, L: độ dài câu tối đa, D: chiều embedding)\n        # doc_lens: danh sách độ dài tài liệu (số lượng câu trong mỗi tài liệu)\n    \n        # Tính toán độ dài câu (số lượng token khác 0)\n        sent_lens = torch.sum(torch.sign(x), dim=1).data\n        H = self.hidden_size\n    \n        # Embedding các token\n        x = self.embed(x)  # (N, L, D)\n    \n        # Word-level CNN: áp dụng các lớp CNN và max pooling\n        x = [conv(x.permute(0, 2, 1)) for conv in self.convs]\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n        x = torch.cat(x, 1)\n    \n        # Padding tài liệu để có độ dài bằng nhau\n        x = self.pad_doc(x, doc_lens)\n    \n        # Sentence-level GRU: áp dụng GRU ở cấp câu\n        sent_out = self.sent_RNN(x)[0]  # (B, max_doc_len, 2*H)\n    \n        # Document-level Max Pooling: tạo vector biểu diễn tài liệu\n        docs = self.max_pool1d(sent_out, doc_lens)  # (B, 2*H)\n    \n        # Fully Connected Layer: biến đổi vector biểu diễn tài liệu\n        docs = self.fc(docs)\n    \n        probs = []\n        for index, doc_len in enumerate(doc_lens):\n            # Lấy các vector biểu diễn câu hợp lệ\n            valid_hidden = sent_out[index, :doc_len, :]  # (doc_len, 2*H)\n            doc = docs[index].unsqueeze(0)\n            s = Variable(torch.zeros(1, 2*H))\n            if self.device is not None:\n                s = s.cuda()\n    \n            for position, h in enumerate(valid_hidden):\n                h = h.view(1, -1)  # (1, 2*H)\n    \n                # Tính toán positional embeddings (absolute và relative)\n                abs_index = Variable(torch.LongTensor([[position]]))\n                if self.device is not None:\n                    abs_index = abs_index.cuda()\n                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n    \n                rel_index = int(round((position + 1) * 9.0 / doc_len))\n                rel_index = Variable(torch.LongTensor([[rel_index]]))\n                if self.device is not None:\n                    rel_index = rel_index.cuda()\n                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n    \n                # Tính toán điểm số cho câu\n                content = self.content(h)\n                salience = self.salience(h, doc)\n                novelty = -1 * self.novelty(h, F.tanh(s))\n                abs_p = self.abs_pos(abs_features)\n                rel_p = self.rel_pos(rel_features)\n    \n                # Tính toán xác suất chọn câu\n                prob = F.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)\n                s = s + torch.mm(prob, h)\n                probs.append(prob)\n    \n        # Ghép các xác suất và trả về kết quả\n        return torch.cat(probs).squeeze()","metadata":{"execution":{"iopub.status.busy":"2025-04-26T01:58:50.285617Z","iopub.execute_input":"2025-04-26T01:58:50.285912Z","iopub.status.idle":"2025-04-26T01:58:50.302171Z","shell.execute_reply.started":"2025-04-26T01:58:50.285872Z","shell.execute_reply":"2025-04-26T01:58:50.301259Z"},"id":"oLif9alPsCyr","papermill":{"duration":0.022758,"end_time":"2025-02-26T02:28:58.838245","exception":false,"start_time":"2025-02-26T02:28:58.815487","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"id":"e2a04d5a","cell_type":"markdown","source":"## Build RNN-RNN","metadata":{"id":"T8uqNLtNwoRJ","papermill":{"duration":0.005599,"end_time":"2025-02-26T02:28:58.849643","exception":false,"start_time":"2025-02-26T02:28:58.844044","status":"completed"},"tags":[]}},{"id":"56f949af","cell_type":"code","source":"class RNN_RNN(BasicModule):\n    def __init__(self, device, embed_num, embed_dim, hidden_size, seg_num, pos_num, pos_dim):\n        super(RNN_RNN, self).__init__(device)\n\n        self.model_name = 'RNN_RNN'\n        self.device = device\n        self.hidden_size = hidden_size\n\n        # Cài đặt thông số cho các biến khác\n        V = embed_num  # Số lượng từ vựng\n        D = embed_dim  # Kích thước embedding của từ\n        H = hidden_size # Kích thước ẩn của RNN\n        S = seg_num   # Số lượng segment (vị trí tương đối)\n        P_V = pos_num # Số lượng vị trí tuyệt đối\n        P_D = pos_dim # Kích thước embedding vị trí\n\n        # Embedding cho vị trí tuyệt đối và tương đối\n        self.abs_pos_embed = nn.Embedding(P_V, P_D)\n        self.rel_pos_embed = nn.Embedding(S, P_D)\n\n        # Embedding cho từ\n        self.embed = nn.Embedding(V, D, padding_idx=0)\n        # Nếu có embedding được khởi tạo trước, copy vào lớp embedding\n        if embed is not None:\n            self.embed.weight.data.copy_(embed)\n\n        # RNN cấp từ (word-level RNN)\n        self.word_RNN = nn.GRU(\n            input_size=D,\n            hidden_size=H,\n            batch_first=True,\n            bidirectional=True\n        )\n\n        # RNN cấp câu (sentence-level RNN)\n        self.sent_RNN = nn.GRU(\n            input_size=2*H,\n            hidden_size=H,\n            batch_first=True,\n            bidirectional=True\n        )\n\n        # Lớp fully connected để biến đổi vector biểu diễn tài liệu\n        self.fc = nn.Linear(2*H, 2*H)\n\n        # Parameters of Classification Layer (tham số của lớp phân loại)\n        self.content = nn.Linear(2*H, 1, bias=False) # Tính điểm nội dung\n        self.salience = nn.Bilinear(2*H, 2*H, 1, bias=False) # Tính điểm độ quan trọng\n        self.novelty = nn.Bilinear(2*H, 2*H, 1, bias=False) # Tính điểm độ mới\n        self.abs_pos = nn.Linear(P_D, 1, bias=False) # Tính điểm vị trí tuyệt đối\n        self.rel_pos = nn.Linear(P_D, 1, bias=False) # Tính điểm vị trí tương đối\n        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1, 0.1)) # Bias cho lớp phân loại\n\n    def max_pool1d(self, x, seq_lens):\n        # x:[N, L, O_in] (N: batch size, L: độ dài chuỗi tối đa, O_in: số chiều kênh đầu vào)\n        out = []\n        for index, t in enumerate(x):\n            # Lấy phần dữ liệu hợp lệ của chuỗi (loại bỏ padding)\n            t = t[:seq_lens[index], :]\n            # Chuyển vị và thêm chiều để phù hợp với F.max_pool1d\n            t = torch.t(t).unsqueeze(0)\n            # Thực hiện max pooling 1D và thêm vào danh sách kết quả\n            out.append(F.max_pool1d(t, t.size(2)))\n\n        # Ghép các kết quả và loại bỏ chiều không cần thiết\n        out = torch.cat(out).squeeze(2)\n        return out\n\n    def avg_pool1d(self, x, seq_lens):\n        # Tương tự như max_pool1d, nhưng sử dụng average pooling\n        out = []\n        for index, t in enumerate(x):\n            t = t[:seq_lens[index], :]\n            t = torch.t(t).unsqueeze(0)\n            out.append(F.avg_pool1d(t, t.size(2)))\n\n        out = torch.cat(out).squeeze(2)\n        return out\n\n    def forward(self, x, doc_lens):\n        # x:[N, L, D] (N: batch size, L: độ dài câu tối đa, D: chiều embedding)\n        # doc_lens: danh sách độ dài tài liệu (số lượng câu trong mỗi tài liệu)\n\n        # Tính toán độ dài câu (số lượng token khác 0)\n        sent_lens = torch.sum(torch.sign(x), dim=1).data\n\n        # Embedding các token\n        x = self.embed(x)  # (N, L, D)\n\n        # Word-level GRU: áp dụng GRU ở cấp từ\n        H = self.hidden_size\n        x = self.word_RNN(x)[0]  # (N, L, 2*H)\n\n        # Max pooling trên kết quả của word-level GRU\n        word_out = self.max_pool1d(x, sent_lens)\n\n        # Padding tài liệu để có độ dài bằng nhau\n        x = self.pad_doc(word_out, doc_lens)\n\n        # Sentence-level GRU: áp dụng GRU ở cấp câu\n        sent_out = self.sent_RNN(x)[0]  # (B, max_doc_len, 2*H)\n\n        # Max pooling trên kết quả của sentence-level GRU\n        docs = self.max_pool1d(sent_out, doc_lens)  # (B, 2*H)\n\n        probs = []\n        for index, doc_len in enumerate(doc_lens):\n            # Lấy các vector biểu diễn câu hợp lệ\n            valid_hidden = sent_out[index, :doc_len, :]  # (doc_len, 2*H)\n            # Áp dụng lớp fully connected và hàm tanh cho vector biểu diễn tài liệu\n            doc = F.tanh(self.fc(docs[index])).unsqueeze(0)\n            s = Variable(torch.zeros(1, 2*H))\n            if self.device is not None:\n                s = s.cuda()\n\n            for position, h in enumerate(valid_hidden):\n                h = h.view(1, -1)  # (1, 2*H)\n\n                # Tính toán positional embeddings (absolute và relative)\n                abs_index = Variable(torch.LongTensor([[position]]))\n                if self.device is not None:\n                    abs_index = abs_index.cuda()\n                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n\n                rel_index = int(round((position + 1) * 9.0 / doc_len))\n                rel_index = Variable(torch.LongTensor([[rel_index]]))\n                if self.device is not None:\n                    rel_index = rel_index.cuda()\n                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n\n                # Tính toán điểm số cho câu (classification layer)\n                content = self.content(h)\n                salience = self.salience(h, doc)\n                novelty = -1 * self.novelty(h, F.tanh(s))\n                abs_p = self.abs_pos(abs_features)\n                rel_p = self.rel_pos(rel_features)\n\n                # Tính toán xác suất chọn câu\n                prob = F.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)\n                s = s + torch.mm(prob, h)\n                probs.append(prob)\n\n        # Ghép các xác suất và trả về kết quả\n        return torch.cat(probs).squeeze()","metadata":{"execution":{"iopub.status.busy":"2025-04-26T01:58:50.664302Z","iopub.execute_input":"2025-04-26T01:58:50.664525Z","iopub.status.idle":"2025-04-26T01:58:50.677614Z","shell.execute_reply.started":"2025-04-26T01:58:50.664506Z","shell.execute_reply":"2025-04-26T01:58:50.676819Z"},"id":"BoMgLGZlwkjA","papermill":{"duration":0.020233,"end_time":"2025-02-26T02:28:58.875726","exception":false,"start_time":"2025-02-26T02:28:58.855493","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":5},{"id":"2df89be4","cell_type":"markdown","source":"# Setup variabel","metadata":{"id":"XAjJrJfV3IyQ","papermill":{"duration":0.00555,"end_time":"2025-02-26T02:28:58.887060","exception":false,"start_time":"2025-02-26T02:28:58.881510","status":"completed"},"tags":[]}},{"id":"2c614d4a","cell_type":"code","source":"import logging\n\n# Thiết lập logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s [INFO] %(message)s')\nroot_dir = \"/kaggle/working\"\n\n# Thiết lập các biến\nsave_dir = f'{root_dir}/checkpoints/'\nembed_dim = 100\nembed_num = 100\npos_dim = 50\npos_num = 100\nseg_num = 10\nkernel_num = 100\nkernel_sizes = [3,4,5]\nmodel = 'RNN_RNN'\nhidden_size = 200\nvocab_dir = f'{root_dir}/data/vocab.npz'\n\n# Train\nlr = 1e-3\nbatch_size = 32\nepochs = 6\nseed = 1\ntrain_dir = f'{root_dir}/data/train.json'\nval_dir = f'{root_dir}/data/val.json'\nembedding_dir = f'{root_dir}/data/embedding.npz'\nword2id_dir = f'{root_dir}/data/word2id.json'\nreport_every = 1500\nseq_trunc = 50\nmax_norm = 1.0\n\n# Test\nload_dir = f'{root_dir}/checkpoints/RNN_RNN_seed_1.pt'\ntest_dir = f'{root_dir}/data/test.json'\nref_dir = f'{root_dir}/outputs/ref'\nhyp_dir = f'{root_dir}/outputs/hyp'\nfilename_dir = f'{root_dir}/x.txt'  # TextFile to be summarized\ntopk = 3\n\n# Device\ndevice = 0  # Giả sử bạn sử dụng GPU với id 0, hoặc đặt None nếu dùng CPU\n\n# Option flags\ntest = True  # Set True nếu bạn muốn kích hoạt chế độ test\ndebug = False  # Set True nếu bạn muốn bật chế độ debug\npredict = False  # Set True nếu bạn muốn bật chế độ dự đoán\n\n# Kiểm tra xem có sử dụng GPU không\nuse_gpu = device is not None\n\n# In các biến ra để kiểm tra\nprint(f\"Save Directory: {save_dir}\")\nprint(f\"Embedding Dimension: {embed_dim}\")\nprint(f\"Use GPU: {use_gpu}\")\nprint(f\"Test Mode: {test}\")\nprint(f\"Debug Mode: {debug}\")\n\n\n\n# Kiểm tra nếu có GPU và nếu không sử dụng GPU thì cảnh báo\nif torch.cuda.is_available() and not use_gpu:\n    print(\"WARNING: You have a CUDA device, should run with -device 0\")\n\n# Đặt GPU và seed\nif use_gpu:\n    torch.cuda.set_device(device)  # Sử dụng giá trị 'device' đã đặt trước đó\n    torch.cuda.manual_seed(seed)   # Sử dụng giá trị 'seed' đã đặt trước đó\n\ntorch.manual_seed(seed)\nrandom.seed(seed)\nnumpy.random.seed(seed)\n\n# In ra thông tin nếu cần để kiểm tra\nprint(f\"Using GPU: {use_gpu}, Device ID: {device if use_gpu else 'None'}\")\nprint(f\"Random Seed: {seed}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-26T01:58:51.167266Z","iopub.execute_input":"2025-04-26T01:58:51.167532Z","iopub.status.idle":"2025-04-26T01:58:51.283757Z","shell.execute_reply.started":"2025-04-26T01:58:51.167509Z","shell.execute_reply":"2025-04-26T01:58:51.283145Z"},"id":"mgYLrn3U3F51","outputId":"6d243c1d-5687-435c-c33d-c6dd11374a97","papermill":{"duration":0.08834,"end_time":"2025-02-26T02:28:58.981233","exception":false,"start_time":"2025-02-26T02:28:58.892893","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Save Directory: /kaggle/working/checkpoints/\nEmbedding Dimension: 100\nUse GPU: True\nTest Mode: True\nDebug Mode: False\nUsing GPU: True, Device ID: 0\nRandom Seed: 1\n","output_type":"stream"}],"execution_count":6},{"id":"07d5dd22","cell_type":"markdown","source":"# Preprocessing Data","metadata":{"id":"Q3uk3AFRza4H","papermill":{"duration":0.005678,"end_time":"2025-02-26T02:28:58.993145","exception":false,"start_time":"2025-02-26T02:28:58.987467","status":"completed"},"tags":[]}},{"id":"c7378703-6e8f-40d7-ba14-bf1609dfcb97","cell_type":"code","source":"!gdown --id 1JgsboIAs__r6XfCbkDWgmberXJw8FBWE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T01:58:51.812880Z","iopub.execute_input":"2025-04-26T01:58:51.813122Z","iopub.status.idle":"2025-04-26T01:58:59.518102Z","shell.execute_reply.started":"2025-04-26T01:58:51.813103Z","shell.execute_reply":"2025-04-26T01:58:59.517117Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1JgsboIAs__r6XfCbkDWgmberXJw8FBWE\nFrom (redirected): https://drive.google.com/uc?id=1JgsboIAs__r6XfCbkDWgmberXJw8FBWE&confirm=t&uuid=8d6d8bb0-2b68-4ba3-b100-a8dd1e1945b8\nTo: /kaggle/working/data.tar.gz\n100%|████████████████████████████████████████| 375M/375M [00:04<00:00, 76.7MB/s]\n","output_type":"stream"}],"execution_count":7},{"id":"ffb2da49-b630-4fc9-8d5d-9fe468a847af","cell_type":"code","source":"!tar -xvzf /kaggle/working/data.tar.gz -C /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T01:58:59.519614Z","iopub.execute_input":"2025-04-26T01:58:59.520022Z","iopub.status.idle":"2025-04-26T01:59:07.373089Z","shell.execute_reply.started":"2025-04-26T01:58:59.519990Z","shell.execute_reply":"2025-04-26T01:59:07.371882Z"}},"outputs":[{"name":"stdout","text":"data/\ndata/embedding.npz\ndata/test.json\ndata/train.json\ndata/val.json\ndata/word2id.json\n","output_type":"stream"}],"execution_count":8},{"id":"3fa3987d-60cd-4142-b78e-35cfda9434a6","cell_type":"code","source":"with open(train_dir) as f: # Tải dữ liệu train\n    train_examples = [json.loads(line) for line in f]\nlen(train_examples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T01:59:07.375103Z","iopub.execute_input":"2025-04-26T01:59:07.375342Z","iopub.status.idle":"2025-04-26T01:59:11.066823Z","shell.execute_reply.started":"2025-04-26T01:59:07.375321Z","shell.execute_reply":"2025-04-26T01:59:11.065956Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"193983"},"metadata":{}}],"execution_count":9},{"id":"291d434d-390d-4402-a3f6-4916a92cb829","cell_type":"code","source":"train_examples[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T01:59:11.068101Z","iopub.execute_input":"2025-04-26T01:59:11.068381Z","iopub.status.idle":"2025-04-26T01:59:11.073310Z","shell.execute_reply.started":"2025-04-26T01:59:11.068361Z","shell.execute_reply":"2025-04-26T01:59:11.072279Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'doc': \"by thomas durante a newly - released surveillance video taken from inside an oklahoma prison shows a former crime boss and key federal witness lured to his death by fellow inmates\\nsix inmates have been charged with murder in what looks like a pre-planned attack in which they worked together , and even celebrated afterwards\\nmollman , a former motorcycle gang member and admitted drug trafficker , was found in a cell at the grady county on may 17 , 2012\\nhe had pleaded guilty to various drug charges and was a key witness for the government in an ongoing drug investigation\\nthe six inmates charged in mollman 's murder have been identified as sitlington , grant curry , matthew jackson , joseph hill , jerry gonzales and steven nidey\\nsitlington , whom grady county assistant district attorney leah edwards calls one of the primary attackers , is the first inmate to appear before a judge\\nhis next court date is next month\\nshe told news 9 : ' although much of what went on is obscured by the shutting of doors we still have a clearer picture of how the incident occurred and who the major players were\\n' coordinated kill : six inmates have been charged with murder in what looks like a pre-planned attack where they worked together , and even celebrated afterwards defense : at one point , mollman emerges from the cell , appearing to fight off his attackers death : after the inmates come out of the cell again , mollman staggers before collapsing the video begins with two inmates standing around a common area , appearing to watch tv\\ntwo more convicts then enters the frame , with one of them checking out doors and looking around the room\\nhe then jumps on a table , signaling another inmate inside a cell on the second level\\nbrutality : the inmates can than be seen hoisting mollman 's body and dragging him into an adjacent cell as you were : with mollman lying motionless , some of the inmates continue watching tv as others scramble to cover up the crime not long after the gesture , mollman , bald and dressed in his full prison jumpsuit , enters the shot with two others\\ndead in jail : mollman , an alleged member of the rogue motorcycle club , was arrested last year - the culmination of a 12 - year investigation into the gang they lead mollman up the stairs and into one of the cells\\nwhile little can be seen on the video , it 's clear that mollman and the other inmates are involved in some type of scuffle\\nat one point , mollman emerges with his fists up , fighting back against his attackers in a desperate attempt\\nwhen they come out again , mollman staggers and collapses on the ground\\nthe inmates then drag his lifeless body into one of the cells\\nwith his feet sticking out of the cell door , the inmates appear to cover up the murder , with one of them wiping blood off the floor with a towel\\njail officers responded minutes later and found mollman dead in the cell with cuts on his body and apparently beaten to death\\nwhile they examined mollman 's body , two of the inmates appear to fist bump right outside the cell\\nmollman , an alleged member of the rogue motorcycle club , was arrested last year - the culmination of a 12 - year investigation into the gang\\nhis family had said that he was also attacked a week before his death\\ncleanup : as mollman 's body lies in a doorway , one inmate hands another a towel to wipe away blood celebration : after jail officers arrived to investigate mollman 's death minutes later , two of the inmates appear to fist bump right outside the cell watch video here\",\n 'labels': '1\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n1',\n 'summaries': 'mollman found dead in prison cell last year while he was awaiting sentencing in a drug investigation\\nhe was a key witness for the feds in a plea deal\\ninmates can be seen on video leading mollman to his death in what appears to be a pre-planned and coordinated effort'}"},"metadata":{}}],"execution_count":10},{"id":"24804c10","cell_type":"code","source":"def build_vocab():\n    \"\"\"\n    Xây dựng từ vựng (vocabulary) từ file embedding được cung cấp.\n    \"\"\"\n    print('start building vocab')\n\n    # Định nghĩa các token đặc biệt\n    PAD_IDX = 0  # Index cho token padding\n    UNK_IDX = 1  # Index cho token unknown\n    PAD_TOKEN = 'PAD_TOKEN' # Token padding\n    UNK_TOKEN = 'UNK_TOKEN' # Token unknown\n\n    # Mở file embedding để đọc\n    f = open(embedding_dir)\n    # Đọc dòng đầu tiên để lấy kích thước embedding\n    embed_dim = int(next(f).split()[1])\n\n    # Khởi tạo OrderedDict để lưu từ và index của chúng\n    word2id = OrderedDict()\n\n    # Thêm các token đặc biệt vào từ vựng\n    word2id[PAD_TOKEN] = PAD_IDX\n    word2id[UNK_TOKEN] = UNK_IDX\n\n    # Khởi tạo list để lưu các vector embedding\n    embed_list = []\n    # Thêm vector 0 cho PAD và UNK\n    embed_list.append([0 for _ in range(embed_dim)])\n    embed_list.append([0 for _ in range(embed_dim)])\n\n    # Xây dựng từ vựng từ file embedding\n    for line in f:\n        tokens = line.split()\n        # Lấy từ và vector embedding\n        word = tokens[:-1*embed_dim][0]\n        vector = [float(num) for num in tokens[-1*embed_dim:]]\n        # Thêm vector embedding vào list\n        embed_list.append(vector)\n        # Thêm từ vào từ vựng với index tương ứng\n        word2id[word] = len(word2id)\n    # Đóng file embedding\n    f.close()\n    # Chuyển list embedding thành numpy array\n    embed = np.array(embed_list, dtype=np.float32)\n    # Lưu numpy array embedding vào file npy\n    np.savez_compressed(file=vocab_dir, embedding=embed)\n    # Lưu từ vựng (word2id) vào file json\n    with open(word2id_dir, 'w') as f:\n        json.dump(word2id, f)\n\ndef worker(files):\n    \"\"\"\n    Xử lý một nhóm các file dữ liệu để tạo ra các ví dụ (examples).\n    \"\"\"\n    examples = []\n    for f in files:\n        # Đọc nội dung file và chia thành các phần\n        parts = open(f, encoding='latin-1').read().split('\\n\\n')\n        try:\n            # Lấy các entities từ phần cuối của file\n            entities = {line.strip().split(':')[0]: line.strip().split(':')[1].lower() for line in parts[-1].split('\\n')}\n        except:\n            # Nếu có lỗi khi xử lý entities, bỏ qua file này\n            continue\n        sents, labels, summaries = [], [], []\n        # Xử lý phần nội dung (content)\n        for line in parts[1].strip().split('\\n'):\n            content, label = line.split('\\t\\t\\t')\n            tokens = content.strip().split()\n            # Thay thế các entities trong tokens bằng giá trị tương ứng\n            for i, token in enumerate(tokens):\n                if token in entities:\n                    tokens[i] = entities[token]\n            # Chuyển label thành '1' hoặc '0'\n            label = '1' if label == '1' else '0'\n            # Thêm câu và label vào list\n            sents.append(' '.join(tokens))\n            labels.append(label)\n        # Xử lý phần summary\n        for line in parts[2].strip().split('\\n'):\n            tokens = line.strip().split()\n            # Thay thế các entities trong tokens bằng giá trị tương ứng\n            for i, token in enumerate(tokens):\n                if token in entities:\n                    tokens[i] = entities[token]\n            # Loại bỏ ký tự '*' và thêm summary vào list\n            line = ' '.join(tokens).replace('*', '')\n            summaries.append(line)\n        # Tạo example dictionary và thêm vào list examples\n        ex = {'doc': '\\n'.join(sents), 'labels': '\\n'.join(labels), 'summaries': '\\n'.join(summaries)}\n        examples.append(ex)\n    return examples\n\ndef build_dataset(args):\n    \"\"\"\n    Xây dựng dataset từ các file dữ liệu sử dụng multiprocessing.\n    \"\"\"\n    t1 = time()\n\n    print('start building dataset')\n    # Cảnh báo nếu worker_num là 1 và có nhiều CPU\n    if worker_num == 1 and cpu_count() > 1:\n        print('[INFO] There are %d CPUs in your device, please increase -worker_num to speed up' % (cpu_count()))\n        print(\"    It's a IO intensive application, so 2~10 may be a good choise\")\n\n    # Lấy danh sách các file dữ liệu\n    files = glob(args.source_dir)\n    data_num = len(files)\n    # Chia các file thành các nhóm cho từng worker\n    group_size = data_num // worker_num\n    groups = []\n    for i in range(worker_num):\n        if i == worker_num - 1:\n            groups.append(files[i*group_size:])\n        else:\n            groups.append(files[i*group_size: (i+1)*group_size])\n    # Tạo pool các worker\n    p = Pool(processes=worker_num)\n    # Gửi các nhóm file cho các worker để xử lý\n    multi_res = [p.apply_async(worker, (fs,)) for fs in groups]\n    # Lấy kết quả từ các worker\n    res = [res.get() for res in multi_res]\n\n    # Ghi kết quả vào file json\n    with open(target_dir, 'w') as f:\n        for row in chain(*res):\n            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n\n    t2 = time()\n    print('Time Cost : %.1f seconds' % (t2 - t1))","metadata":{"execution":{"iopub.status.busy":"2025-04-26T01:59:11.074474Z","iopub.execute_input":"2025-04-26T01:59:11.074783Z","iopub.status.idle":"2025-04-26T01:59:11.484837Z","shell.execute_reply.started":"2025-04-26T01:59:11.074753Z","shell.execute_reply":"2025-04-26T01:59:11.484166Z"},"id":"HuufWkKjzahI","papermill":{"duration":0.020582,"end_time":"2025-02-26T02:28:59.019726","exception":false,"start_time":"2025-02-26T02:28:58.999144","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":11},{"id":"2e6e4a7d","cell_type":"markdown","source":"## Get feature from word2id","metadata":{"id":"kqVLuwJ6_bre","papermill":{"duration":0.005623,"end_time":"2025-02-26T02:28:59.031273","exception":false,"start_time":"2025-02-26T02:28:59.025650","status":"completed"},"tags":[]}},{"id":"e1923006","cell_type":"code","source":"class Vocab():\n    def __init__(self, embed, word2id):\n        \"\"\"\n        Khởi tạo đối tượng Vocab.\n\n        Args:\n            embed (numpy.ndarray): Ma trận embedding từ vựng.\n            word2id (dict): Từ điển ánh xạ từ từ sang index.\n        \"\"\"\n        self.embed = embed\n        self.word2id = word2id\n        self.id2word = {v: k for k, v in word2id.items()}  # Tạo từ điển ánh xạ từ index sang từ\n        assert len(self.word2id) == len(self.id2word)  # Đảm bảo kích thước từ điển nhất quán\n        self.PAD_IDX = 0  # Index cho token padding\n        self.UNK_IDX = 1  # Index cho token unknown\n        self.PAD_TOKEN = 'PAD_TOKEN'  # Token padding\n        self.UNK_TOKEN = 'UNK_TOKEN'  # Token unknown\n\n    def __len__(self):\n        \"\"\"\n        Trả về kích thước từ vựng.\n        \"\"\"\n        return len(self.word2id)\n\n    def i2w(self, idx):\n        \"\"\"\n        Chuyển đổi index thành từ.\n\n        Args:\n            idx (int): Index của từ.\n\n        Returns:\n            str: Từ tương ứng với index.\n        \"\"\"\n        return self.id2word[idx]\n\n    def w2i(self, w):\n        \"\"\"\n        Chuyển đổi từ thành index.\n\n        Args:\n            w (str): Từ cần chuyển đổi.\n\n        Returns:\n            int: Index của từ, hoặc UNK_IDX nếu từ không có trong từ vựng.\n        \"\"\"\n        if w in self.word2id:\n            return self.word2id[w]\n        else:\n            return self.UNK_IDX\n\n    def make_features(self, batch, sent_trunc=50, doc_trunc=100, split_token='\\n'):\n        \"\"\"\n        Tạo features từ batch dữ liệu cho quá trình huấn luyện.\n\n        Args:\n            batch (dict): Batch dữ liệu chứa các trường 'doc', 'labels', 'summaries'.\n            sent_trunc (int): Độ dài tối đa của câu.\n            doc_trunc (int): Số lượng câu tối đa trong một tài liệu.\n            split_token (str): Token dùng để tách câu trong tài liệu và labels.\n\n        Returns:\n            tuple: Tuple chứa features, targets, summaries, và doc_lens.\n                - features (torch.LongTensor): Tensor chứa index của các từ trong câu.\n                - targets (torch.LongTensor): Tensor chứa labels của các câu.\n                - summaries (list): List chứa tóm tắt của các tài liệu.\n                - doc_lens (list): List chứa độ dài của các tài liệu (số lượng câu).\n        \"\"\"\n        sents_list, targets, doc_lens = [], [], []\n        # Truncate document\n        for doc, label in zip(batch['doc'], batch['labels']):\n            sents = doc.split(split_token)\n            labels = label.split(split_token)\n            labels = [int(l) for l in labels]\n            max_sent_num = min(doc_trunc, len(sents))\n            sents = sents[:max_sent_num]\n            labels = labels[:max_sent_num]\n            sents_list += sents\n            targets += labels\n            doc_lens.append(len(sents))\n        # Truncate or pad sentence\n        max_sent_len = 0\n        batch_sents = []\n        for sent in sents_list:\n            words = sent.split()\n            if len(words) > sent_trunc:\n                words = words[:sent_trunc]\n            max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n            batch_sents.append(words)\n\n        features = []\n        for sent in batch_sents:\n            feature = [self.w2i(w) for w in sent] + [self.PAD_IDX for _ in range(max_sent_len - len(sent))]\n            features.append(feature)\n\n        features = torch.LongTensor(features)\n        targets = torch.LongTensor(targets)\n        summaries = batch['summaries']\n\n        return features, targets, summaries, doc_lens\n\n    def make_predict_features(self, batch, sent_trunc=150, doc_trunc=100, split_token='. '):\n        \"\"\"\n        Tạo features từ batch dữ liệu cho quá trình dự đoán.\n\n        Args:\n            batch (list): List chứa các tài liệu (mỗi tài liệu là một chuỗi).\n            sent_trunc (int): Độ dài tối đa của câu.\n            doc_trunc (int): Số lượng câu tối đa trong một tài liệu.\n            split_token (str): Token dùng để tách câu trong tài liệu.\n\n        Returns:\n            tuple: Tuple chứa features và doc_lens.\n                - features (torch.LongTensor): Tensor chứa index của các từ trong câu.\n                - doc_lens (list): List chứa độ dài của các tài liệu (số lượng câu).\n        \"\"\"\n        sents_list, doc_lens = [], []\n        for doc in batch:\n            sents = doc.split(split_token)\n            max_sent_num = min(doc_trunc, len(sents))\n            sents = sents[:max_sent_num]\n            sents_list += sents\n            doc_lens.append(len(sents))\n        # Truncate or pad sentence\n        max_sent_len = 0\n        batch_sents = []\n        for sent in sents_list:\n            words = sent.split()\n            if len(words) > sent_trunc:\n                words = words[:sent_trunc]\n            max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n            batch_sents.append(words)\n\n        features = []\n        for sent in batch_sents:\n            feature = [self.w2i(w) for w in sent] + [self.PAD_IDX for _ in range(max_sent_len - len(sent))]\n            features.append(feature)\n\n        features = torch.LongTensor(features)\n\n        return features, doc_lens","metadata":{"execution":{"iopub.status.busy":"2025-04-26T01:59:11.485763Z","iopub.execute_input":"2025-04-26T01:59:11.486081Z","iopub.status.idle":"2025-04-26T01:59:11.504848Z","shell.execute_reply.started":"2025-04-26T01:59:11.486050Z","shell.execute_reply":"2025-04-26T01:59:11.504130Z"},"id":"IWGYrdZZ_bT3","papermill":{"duration":0.08977,"end_time":"2025-02-26T02:28:59.126880","exception":false,"start_time":"2025-02-26T02:28:59.037110","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":12},{"id":"427f87f9","cell_type":"markdown","source":"## Custom Dataset","metadata":{"id":"d7CSeoXVAOE_","papermill":{"duration":0.005728,"end_time":"2025-02-26T02:28:59.138941","exception":false,"start_time":"2025-02-26T02:28:59.133213","status":"completed"},"tags":[]}},{"id":"56bcc6aa","cell_type":"code","source":"class Dataset(data.Dataset):\n    def __init__(self, examples):\n        super(Dataset,self).__init__()\n        # data: {'sents':xxxx,'labels':'xxxx', 'summaries':[1,0]}\n        self.examples = examples\n        self.training = False\n    def train(self):\n        self.training = True\n        return self\n    def test(self):\n        self.training = False\n        return self\n    def shuffle(self,words):\n        np.random.shuffle(words)\n        return ' '.join(words)\n    def dropout(self,words,p=0.3):\n        l = len(words)\n        drop_index = np.random.choice(l,int(l*p))\n        keep_words = [words[i] for i in range(l) if i not in drop_index]\n        return ' '.join(keep_words)\n    def __getitem__(self, idx):\n        ex = self.examples[idx]\n        return ex\n\n    def __len__(self):\n        return len(self.examples)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-26T01:59:11.505729Z","iopub.execute_input":"2025-04-26T01:59:11.506045Z","iopub.status.idle":"2025-04-26T01:59:11.521187Z","shell.execute_reply.started":"2025-04-26T01:59:11.506026Z","shell.execute_reply":"2025-04-26T01:59:11.520432Z"},"id":"xoXZ_Kmf_6-k","papermill":{"duration":0.012682,"end_time":"2025-02-26T02:28:59.157531","exception":false,"start_time":"2025-02-26T02:28:59.144849","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":13},{"id":"1bf5378d","cell_type":"markdown","source":"# Main","metadata":{"id":"quXj8MD36qCw","papermill":{"duration":0.005717,"end_time":"2025-02-26T02:28:59.169172","exception":false,"start_time":"2025-02-26T02:28:59.163455","status":"completed"},"tags":[]}},{"id":"74a9c0a0","cell_type":"markdown","source":"## Training","metadata":{"papermill":{"duration":0.005771,"end_time":"2025-02-26T02:28:59.180917","exception":false,"start_time":"2025-02-26T02:28:59.175146","status":"completed"},"tags":[]}},{"id":"d15e5716","cell_type":"code","source":"\ndef eval(net, vocab, data_iter, criterion):\n    \"\"\"\n    Đánh giá mô hình trên tập dữ liệu.\n\n    Args:\n        net: Mô hình mạng nơ-ron.\n        vocab: Từ điển.\n        data_iter: Dữ liệu đánh giá.\n        criterion: Hàm mất mát.\n\n    Returns:\n        Mất mát trung bình.\n    \"\"\"\n    net.eval()  # Chuyển sang chế độ đánh giá\n    total_loss = 0\n    batch_num = 0\n\n    for batch in data_iter:\n        features, targets, _, doc_lens = vocab.make_features(batch) # Tạo đặc trưng\n        features, targets = Variable(features), Variable(targets.float()) # Chuyển đổi sang biến\n        if use_gpu: # Chuyển sang GPU\n            features = features.cuda()\n            targets = targets.cuda()\n        probs = net(features, doc_lens) # Dự đoán\n        loss = criterion(probs, targets) # Tính toán mất mát\n        total_loss += loss.item()\n        batch_num += 1\n\n    loss = total_loss / batch_num # Tính mất mát trung bình\n    net.train() # Chuyển lại chế độ huấn luyện\n    return loss","metadata":{"execution":{"iopub.status.busy":"2025-04-26T01:59:11.522781Z","iopub.execute_input":"2025-04-26T01:59:11.523039Z","iopub.status.idle":"2025-04-26T01:59:11.534537Z","shell.execute_reply.started":"2025-04-26T01:59:11.523019Z","shell.execute_reply":"2025-04-26T01:59:11.533809Z"},"papermill":{"duration":0.011895,"end_time":"2025-02-26T02:28:59.198578","exception":false,"start_time":"2025-02-26T02:28:59.186683","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":14},{"id":"c31eb116","cell_type":"code","source":"# print(embedding_dir)  # In đường dẫn embedding\n# print(kernel_sizes)  # In kích thước kernel\n\nlogging.info('Loading vocab,train and val dataset.Wait a second,please') # Thông báo tải dữ liệu\n\nembed = torch.Tensor(np.load(embedding_dir)['embedding']) # Tải embedding\nwith open(word2id_dir) as f: # Tải word2id\n    word2id = json.load(f)\nvocab = Vocab(embed, word2id) # Tạo từ điển\n\nwith open(train_dir) as f: # Tải dữ liệu train\n    train_examples = [json.loads(line) for line in f]\ntrain_dataset = Dataset(train_examples[:10000]) # Tạo dataset train (10 examples)\n\nwith open(val_dir) as f: # Tải dữ liệu validation\n    val_examples = [json.loads(line) for line in f]\nval_dataset = Dataset(val_examples[0:1000]) # Tạo dataset validation (10 examples)\n\n# update args\nembed_num = embed.size(0) # Lấy số lượng embedding\nembed_dim = embed.size(1) # Lấy kích thước embedding\n# print(kernel_sizes)\nkernel_sizes = [int(ks) for ks in kernel_sizes] # Chuyển đổi kích thước kernel thành int\n\n# build model\n# if you want to build a model with RNN-RNN achitecture\nnet = RNN_RNN(device, embed_num, embed_dim, hidden_size, seg_num, pos_num, pos_dim) # Tạo mô hình RNN-RNN\n# if you want to build a model with CNN-RNN achitecture\n# net = CNN_RNN(device, kernel_sizes, embed_dim, kernel_num, embed_num, hidden_size, seg_num, pos_num, pos_dim) # Tạo mô hình CNN-RNN (đang comment)\n\nif use_gpu: # Chuyển mô hình sang GPU nếu cần\n    net.cuda()\n\n# load dataset\ntrain_iter = DataLoader(dataset=train_dataset, # Tạo DataLoader cho train\n                        batch_size=batch_size,\n                        shuffle=True)\nval_iter = DataLoader(dataset=val_dataset, # Tạo DataLoader cho validation\n                        batch_size=batch_size,\n                        shuffle=False)\n\n# loss function\ncriterion = nn.BCELoss() # Hàm mất mát Binary Cross Entropy\n\n# model info\n# print(net) # In thông tin mô hình\nparams = sum(p.numel() for p in list(net.parameters())) / 1e6 # Tính số lượng tham số\n# print('#Params: %.1fM' % (params)) # In số lượng tham số\n\nmin_loss = float('inf') # Khởi tạo mất mát tối thiểu\noptimizer = torch.optim.Adam(net.parameters(),lr=lr) # Khởi tạo optimizer Adam\nnet.train() # Chuyển mô hình sang chế độ train\n\nt1 = time() # Bắt đầu đo thời gian\nfor epoch in range(1, epochs + 1):  # Loop over epochs\n    net.train()  # Set the model to training mode\n    total_train_loss = 0  # Initialize variable to accumulate training loss\n    for i, batch in enumerate(train_iter):  # Loop over batches in the training data\n        features, targets, _, doc_lens = vocab.make_features(batch)  # Create features\n        features, targets = Variable(features), Variable(targets.float())  # Convert to variables\n        if use_gpu:  # Move to GPU if needed\n            features = features.cuda()\n            targets = targets.cuda()\n        \n        probs = net(features, doc_lens)  # Get predictions\n        loss = criterion(probs, targets)  # Compute the loss\n        optimizer.zero_grad()  # Reset gradients\n        loss.backward()  # Backpropagation\n        clip_grad_norm(net.parameters(), max_norm)  # Clip gradients\n        optimizer.step()  # Update model parameters\n        \n        total_train_loss += loss.item()  # Accumulate the training loss\n        \n        if debug:  # Print batch information if debugging\n            print('Batch ID:%d Loss:%f' % (i, loss.item()))\n            continue\n    \n    # Compute average training loss for the epoch\n    avg_train_loss = total_train_loss / len(train_iter)\n\n    # Evaluate the model on the validation set\n    net.eval()  # Set the model to evaluation mode\n    with torch.no_grad():  # Disable gradient calculation during validation\n        val_loss = eval(net, vocab, val_iter, criterion)  # Evaluate model on validation data\n        if val_loss < min_loss: # Cập nhật mất mát tối thiểu\n                min_loss = val_loss\n                best_path = net.save() # Lưu mô hình tốt nhất\n    \n    # Print training and validation loss\n    print('Epoch%2d Train_Loss: %f' % (epoch, avg_train_loss, ))\nt2 = time() # Kết thúc đo thời gian\nlogging.info('Total Cost:%f h'%((t2-t1)/3600)) # In thời gian huấn luyện","metadata":{"execution":{"iopub.status.busy":"2025-04-26T01:59:11.535440Z","iopub.execute_input":"2025-04-26T01:59:11.535631Z","iopub.status.idle":"2025-04-26T02:53:47.714847Z","shell.execute_reply.started":"2025-04-26T01:59:11.535615Z","shell.execute_reply":"2025-04-26T02:53:47.714063Z"},"id":"Ag80Nz2cLpnB","outputId":"824b00fa-5b00-4cae-d739-428b2272461c","papermill":{"duration":9.779283,"end_time":"2025-02-26T02:29:08.984208","exception":false,"start_time":"2025-02-26T02:28:59.204925","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"<ipython-input-15-b317a4101892>:69: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n  clip_grad_norm(net.parameters(), max_norm)  # Clip gradients\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Train_Loss: 0.513053\nEpoch 2 Train_Loss: 0.424046\nEpoch 3 Train_Loss: 0.366028\nEpoch 4 Train_Loss: 0.279440\nEpoch 5 Train_Loss: 0.178280\nEpoch 6 Train_Loss: 0.099140\n","output_type":"stream"}],"execution_count":15},{"id":"8cbabde3","cell_type":"markdown","source":"## Test","metadata":{"papermill":{"duration":0.006032,"end_time":"2025-02-26T02:29:08.997005","exception":false,"start_time":"2025-02-26T02:29:08.990973","status":"completed"},"tags":[]}},{"id":"5a96f5a3","cell_type":"code","source":"embed = torch.Tensor(np.load(embedding_dir)['embedding']) # Tải embedding\nwith open(word2id_dir) as f: # Tải word2id\n    word2id = json.load(f)\nvocab = Vocab(embed, word2id) # Tạo từ điển\n\nwith open(test_dir) as f: # Tải dữ liệu test\n    examples = [json.loads(line) for line in f]\ntest_dataset = Dataset(examples[0:10]) # Tạo dataset test \n\ntest_iter = DataLoader(dataset=test_dataset, # Tạo DataLoader cho test\n                        batch_size=batch_size,\n                        shuffle=False)\n\nif use_gpu: # Tải checkpoint mô hình (GPU)\n    checkpoint = torch.load(load_dir)\nelse: # Tải checkpoint mô hình (CPU)\n    checkpoint = torch.load(load_dir, map_location=lambda storage, loc: storage)\n\n# checkpoint['args']['device'] saves the device used as train time\n# if at test time, we are using a CPU, we must override device to None\n\nnet = RNN_RNN(device, embed_num, embed_dim, hidden_size, seg_num, pos_num, pos_dim) # Tạo mô hình RNN-RNN\n# if you want to build a model with CNN-RNN achitecture\n# net = CNN_RNN(device, kernel_sizes, embed_dim, kernel_num, embed_num, hidden_size, seg_num, pos_num, pos_dim) # Tạo mô hình CNN-RNN (đang comment)\n\nnet.load_state_dict(checkpoint['model']) # Tải trạng thái mô hình từ checkpoint\nif use_gpu: # Chuyển mô hình sang GPU nếu cần\n    net.cuda()\nnet.eval() # Chuyển mô hình sang chế độ đánh giá\n\ndoc_num = len(test_dataset) # Lấy số lượng tài liệu test\ntime_cost = 0 # Khởi tạo thời gian đánh giá\nfile_id = 1 # Khởi tạo id file kết quả\n\nfor batch in tqdm(test_iter): # Lặp qua các batch test\n    features,_,summaries,doc_lens = vocab.make_features(batch) # Tạo đặc trưng\n    t1 = time() # Bắt đầu đo thời gian dự đoán\n    if use_gpu: # Dự đoán (GPU)\n        probs = net(Variable(features).cuda(), doc_lens)\n    else: # Dự đoán (CPU)\n        probs = net(Variable(features), doc_lens)\n    t2 = time() # Kết thúc đo thời gian dự đoán\n    time_cost += t2 - t1 # Tính tổng thời gian dự đoán\n    start = 0 # Khởi tạo vị trí bắt đầu\n    for doc_id,doc_len in enumerate(doc_lens): # Lặp qua các tài liệu trong batch\n        stop = start + doc_len # Tính vị trí kết thúc\n        prob = probs[start:stop] # Lấy xác suất dự đoán cho tài liệu hiện tại\n        topk = min(topk,doc_len) # Lấy top k câu (tối đa là độ dài tài liệu)\n        topk_indices = prob.topk(topk)[1].cpu().data.numpy() # Lấy indices của top k câu\n        topk_indices.sort() # Sắp xếp indices\n        doc = batch['doc'][doc_id].split('\\n')[:doc_len] # Lấy nội dung tài liệu\n        # print(doc)\n        # print(\"*\" * 20)\n        hyp = [doc[index] for index in topk_indices] # Tạo tóm tắt dự đoán\n        # print(hyp)\n        # print(\"*\" * 20)\n        ref = summaries[doc_id] # Lấy tóm tắt tham chiếu\n        # print(ref)\n        # print(\"-\" * 80)\n\n        os.makedirs(ref_dir, exist_ok=True) # Tạo thư mục tham chiếu nếu chưa tồn tại\n        os.makedirs(hyp_dir, exist_ok=True) # Tạo thư mục dự đoán nếu chưa tồn tại\n        with open(os.path.join(ref_dir,str(file_id)+'.txt'), 'w') as f: # Lưu tóm tắt tham chiếu\n            f.write(ref)\n        with open(os.path.join(hyp_dir,str(file_id)+'.txt'), 'w') as f: # Lưu tóm tắt dự đoán\n            f.write('\\n'.join(hyp))\n        start = stop # Cập nhật vị trí bắt đầu\n        file_id = file_id + 1 # Tăng id file\nprint('Speed: %.2f docs / s' % (doc_num / time_cost)) # In tốc độ đánh giá","metadata":{"execution":{"iopub.status.busy":"2025-04-26T02:57:03.064221Z","iopub.execute_input":"2025-04-26T02:57:03.064659Z","iopub.status.idle":"2025-04-26T02:57:04.109266Z","shell.execute_reply.started":"2025-04-26T02:57:03.064636Z","shell.execute_reply":"2025-04-26T02:57:04.108508Z"},"papermill":{"duration":1.028324,"end_time":"2025-02-26T02:29:10.031476","exception":false,"start_time":"2025-02-26T02:29:09.003152","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"<ipython-input-16-1cb9d56183a6>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(load_dir)\n100%|██████████| 1/1 [00:00<00:00,  4.73it/s]","output_type":"stream"},{"name":"stdout","text":"Speed: 65.27 docs / s\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"id":"8061f41c-4581-470d-8bca-66177f90671e","cell_type":"code","source":"import glob\nwith open(test_dir) as f: # Tải dữ liệu train\n    test_examples = [json.loads(line) for line in f]\n    \ndef read_summaries(hyp_dir, ref_dir, file_id):\n    \"\"\"\n    Đọc lại văn bản gốc, tóm tắt dự đoán và tóm tắt tham chiếu từ các tệp đã lưu.\n    \n    Args:\n        hyp_dir (str): Thư mục chứa tóm tắt dự đoán.\n        ref_dir (str): Thư mục chứa tóm tắt tham chiếu.\n        file_id (int): ID của tệp cần đọc.\n    \n    Returns:\n        tuple: (tóm tắt dự đoán, tóm tắt tham chiếu)\n    \"\"\"\n    hyp_file = os.path.join(hyp_dir, f\"{file_id}.txt\")\n    ref_file = os.path.join(ref_dir, f\"{file_id}.txt\")\n    \n    with open(hyp_file, 'r') as f:\n        hyp_summary = f.read()\n    \n    with open(ref_file, 'r') as f:\n        ref_summary = f.read()\n    \n    return hyp_summary, ref_summary\n\n# Ví dụ đọc lại một tóm tắt\nfile_id_to_read = 1  # Thay đổi giá trị này để đọc file khác\nhyp_text, ref_text = read_summaries(hyp_dir, ref_dir, file_id_to_read)\nprint(\"Văn bản gốc: \")\nprint(test_examples[file_id_to_read - 1])\nprint (\"=\" * 20)\nprint(\"Tóm tắt dự đoán:\")\nprint(hyp_text)\nprint (\"=\" * 20)\nprint(\"\\nTóm tắt tham chiếu:\")\nprint(ref_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T15:47:29.464956Z","iopub.execute_input":"2025-04-25T15:47:29.465299Z","iopub.status.idle":"2025-04-25T15:47:29.645208Z","shell.execute_reply.started":"2025-04-25T15:47:29.465272Z","shell.execute_reply":"2025-04-25T15:47:29.644513Z"}},"outputs":[{"name":"stdout","text":"Văn bản gốc: \n{'doc': \"things have n't been going well for manchester united flop anderson since he left old trafford to return to brazil , but at least on wednesday his own errors were outshone by the stupidity of a team - mate\\nthe brazil midfielder was sent off for internacional after a first - half off - the - ball shove on ypiranga erechim striker neto that also saw his opponent sent off for retaliating with an elbow\\nbut fortunately for anderson , his colleague fabricio stole the limelight by swearing at his own fans and becoming the third player to be shown the red card , reducing internacional to nine men in the 1 - 1 draw in porto alegre\\nformer manchester united midfielder anderson shoved neto off the ball to earn a red card neto also saw red after he retaliated by by appearing to aim an elbow at anderson in porto alegre the two players listen to the referee as he reaches for his card from his back pocket on wednesday night anderson shows his shock to be shown the red card for what he believed to be an innocent challenge while some players offered their hands in apology to the fans after being repeatedly booed , fabricio took a different route and showed two middle fingers to the crowd leaving referee luis teixeira rocha with no choice but to give him his marching orders\\nthe 28 - year - old left back reacted by tearing off his shirt , throwing it to the floor and shouting ' i 'm leaving , i 'm leaving , ' as he walked down the tunnel\\nhe has already been suspended until the end of the week by his club\\nanderson ended his seven - and - a - half year association with manchester united in february by signing a four - year deal with internacional , but his return to brazil has not gone as planned\\ninternacional 's fabricio was also sent off later in the game for aiming his middle fingers at booing fans fabricio threw his shirt to the ground after being shown the red card and shouted ' i 'm leaving , i 'm leaving ' having missed a penalty on his debut , he was substituted later in february after just 36 minutes needing oxygen while playing in the high - altitude estadio hernando siles in la paz\\nwhile internacional sit top of the table in the campeonato gaucho , anderson has faced a familiar struggle for fitness despite making eight appearances since his move\\nanderson ended a miserable seven - and - a - half year spell with manchester united by leaving this season\", 'labels': '1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0', 'summaries': 'manchester united flop anderson was sent off for internacional this week\\nanderson saw red for an off - the - ball shove during the first half of 1 - 1 draw\\nteam - mate fabricio stole the limelight by swearing at his own fans'}\n====================\nTóm tắt dự đoán:\nthings have n't been going well for manchester united flop anderson since he left old trafford to return to brazil , but at least on wednesday his own errors were outshone by the stupidity of a team - mate\nbut fortunately for anderson , his colleague fabricio stole the limelight by swearing at his own fans and becoming the third player to be shown the red card , reducing internacional to nine men in the 1 - 1 draw in porto alegre\nformer manchester united midfielder anderson shoved neto off the ball to earn a red card neto also saw red after he retaliated by by appearing to aim an elbow at anderson in porto alegre the two players listen to the referee as he reaches for his card from his back pocket on wednesday night anderson shows his shock to be shown the red card for what he believed to be an innocent challenge while some players offered their hands in apology to the fans after being repeatedly booed , fabricio took a different route and showed two middle fingers to the crowd leaving referee luis teixeira rocha with no choice but to give him his marching orders\n====================\n\nTóm tắt tham chiếu:\nmanchester united flop anderson was sent off for internacional this week\nanderson saw red for an off - the - ball shove during the first half of 1 - 1 draw\nteam - mate fabricio stole the limelight by swearing at his own fans\n","output_type":"stream"}],"execution_count":25},{"id":"a58fdbb3-9689-45cf-bfd8-8fd86810331b","cell_type":"code","source":"\ntest_examples[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T15:45:46.000687Z","iopub.execute_input":"2025-04-25T15:45:46.001148Z","iopub.status.idle":"2025-04-25T15:45:46.192946Z","shell.execute_reply.started":"2025-04-25T15:45:46.001107Z","shell.execute_reply":"2025-04-25T15:45:46.192221Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'doc': \"things have n't been going well for manchester united flop anderson since he left old trafford to return to brazil , but at least on wednesday his own errors were outshone by the stupidity of a team - mate\\nthe brazil midfielder was sent off for internacional after a first - half off - the - ball shove on ypiranga erechim striker neto that also saw his opponent sent off for retaliating with an elbow\\nbut fortunately for anderson , his colleague fabricio stole the limelight by swearing at his own fans and becoming the third player to be shown the red card , reducing internacional to nine men in the 1 - 1 draw in porto alegre\\nformer manchester united midfielder anderson shoved neto off the ball to earn a red card neto also saw red after he retaliated by by appearing to aim an elbow at anderson in porto alegre the two players listen to the referee as he reaches for his card from his back pocket on wednesday night anderson shows his shock to be shown the red card for what he believed to be an innocent challenge while some players offered their hands in apology to the fans after being repeatedly booed , fabricio took a different route and showed two middle fingers to the crowd leaving referee luis teixeira rocha with no choice but to give him his marching orders\\nthe 28 - year - old left back reacted by tearing off his shirt , throwing it to the floor and shouting ' i 'm leaving , i 'm leaving , ' as he walked down the tunnel\\nhe has already been suspended until the end of the week by his club\\nanderson ended his seven - and - a - half year association with manchester united in february by signing a four - year deal with internacional , but his return to brazil has not gone as planned\\ninternacional 's fabricio was also sent off later in the game for aiming his middle fingers at booing fans fabricio threw his shirt to the ground after being shown the red card and shouted ' i 'm leaving , i 'm leaving ' having missed a penalty on his debut , he was substituted later in february after just 36 minutes needing oxygen while playing in the high - altitude estadio hernando siles in la paz\\nwhile internacional sit top of the table in the campeonato gaucho , anderson has faced a familiar struggle for fitness despite making eight appearances since his move\\nanderson ended a miserable seven - and - a - half year spell with manchester united by leaving this season\",\n 'labels': '1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0',\n 'summaries': 'manchester united flop anderson was sent off for internacional this week\\nanderson saw red for an off - the - ball shove during the first half of 1 - 1 draw\\nteam - mate fabricio stole the limelight by swearing at his own fans'}"},"metadata":{}}],"execution_count":20},{"id":"a7f080e2-6664-43e1-b5e4-c4fbecc64c02","cell_type":"markdown","source":"# Evaluation","metadata":{}},{"id":"d8d19613-1dfc-4831-9d48-97710f0a2ceb","cell_type":"code","source":"!pip install rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T02:57:16.910735Z","iopub.execute_input":"2025-04-26T02:57:16.911045Z","iopub.status.idle":"2025-04-26T02:57:23.189796Z","shell.execute_reply.started":"2025-04-26T02:57:16.911023Z","shell.execute_reply":"2025-04-26T02:57:23.188705Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge-score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge-score) (2024.2.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=b74ec7bb882b558a9c10f016b0a0a43c160419ede80d1d3c93d9f780f3cbbf4b\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":17},{"id":"a0a2d3f8-be80-4ef4-baf8-7c31d5e7264f","cell_type":"code","source":"from rouge_score import rouge_scorer\n\n# Tạo RougeScorer\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n# Danh sách lưu tổng điểm\ntotal_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n\n# Duyệt qua tất cả các tệp trong thư mục tham chiếu\nfor filename in os.listdir(ref_dir):\n    ref_path = os.path.join(ref_dir, filename)\n    hyp_path = os.path.join(hyp_dir, filename)\n\n    # Kiểm tra nếu tệp tồn tại trong cả hai thư mục\n    if os.path.exists(hyp_path):\n        with open(ref_path, 'r', encoding='utf-8') as f:\n            reference = f.read().strip()\n        with open(hyp_path, 'r', encoding='utf-8') as f:\n            hypothesis = f.read().strip()\n\n        # Tính điểm ROUGE cho từng cặp tệp\n        scores = scorer.score(reference, hypothesis)\n\n        # Lưu điểm vào danh sách\n        for key in total_scores:\n            total_scores[key].append(scores[key].fmeasure)  # Lưu giá trị F1-score\n\n# Tính trung bình điểm ROUGE\navg_scores = {key: sum(values) / len(values) for key, values in total_scores.items()}\n\n# In kết quả\nprint(\"ROUGE Average Scores:\")\nprint(f\"ROUGE-1: {avg_scores['rouge1']:.4f}\")\nprint(f\"ROUGE-2: {avg_scores['rouge2']:.4f}\")\nprint(f\"ROUGE-L: {avg_scores['rougeL']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T02:57:25.516749Z","iopub.execute_input":"2025-04-26T02:57:25.517090Z","iopub.status.idle":"2025-04-26T02:57:27.341755Z","shell.execute_reply.started":"2025-04-26T02:57:25.517061Z","shell.execute_reply":"2025-04-26T02:57:27.340928Z"}},"outputs":[{"name":"stdout","text":"ROUGE Average Scores:\nROUGE-1: 0.3625\nROUGE-2: 0.1889\nROUGE-L: 0.2416\n","output_type":"stream"}],"execution_count":18},{"id":"43d7a105-9aaf-493e-b6fa-f5af016bcf08","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9b3241ec-7057-467f-9c60-094a630e9898","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"47e0e61f-a865-4e13-bb80-fa9e37f1a067","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b376313d-3e8b-4b85-ad4c-85dcb9f03a19","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9b655197-e0c1-4362-9c97-35d4021a0b94","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9993c15a-6c50-47e3-a822-b5bb3f29bcef","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3c2b4ad2-f735-4bc6-b0a0-a9fbfacc32a1","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"84d7ad50-1afc-4ac1-bb37-23c52ea8e703","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"13dbc911-ed70-45e9-be0a-4e616e2cfa86","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"56976c56-0a90-4fa1-8439-b600f5b1b203","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b369c37a-4aed-481e-87f4-97b05b769afb","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"40c8c967-eb3d-4f57-a687-d419de581566","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cbb50baa-900a-4bc4-a5ad-b79f5b10eb9b","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2547f0c2-757d-4f89-b940-234c6e2de00e","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3c938d71-2335-4e16-a66c-30af738e7621","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"91db83fd-be50-4113-a5cd-3ef6db158103","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"052a54c6-7700-4646-958c-b19881e663fa","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}